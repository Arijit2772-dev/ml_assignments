{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 3 - Multiple Linear Regression Solutions\n",
    "This notebook contains solutions for all questions (Q1, Q2, Q3) in Lab Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Q1: K-Fold Cross Validation for Multiple Linear Regression (Least Square Error Fit)\n",
    "\n",
    "**Dataset**: USA House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (a): Load dataset and divide into input features and output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load USA Housing dataset\n",
    "try:\n",
    "    df_housing = pd.read_csv('USA_Housing.csv')\n",
    "except:\n",
    "    # Try alternative path\n",
    "    df_housing = pd.read_csv('../Assignment_5_Regression/USA_Housing.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df_housing.shape}\")\n",
    "print(f\"\\nColumns: {df_housing.columns.tolist()}\")\n",
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features (X) and output variable (y - Price)\n",
    "# Remove non-numeric columns if any\n",
    "numeric_cols = df_housing.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "if 'Price' in numeric_cols:\n",
    "    X = df_housing[numeric_cols].drop('Price', axis=1)\n",
    "    y = df_housing['Price']\n",
    "else:\n",
    "    # Use last column as target\n",
    "    X = df_housing[numeric_cols[:-1]]\n",
    "    y = df_housing[numeric_cols[-1]]\n",
    "\n",
    "print(f\"Input Features Shape: {X.shape}\")\n",
    "print(f\"Output Variable Shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (b): Scale the values of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Feature scaling complete!\")\n",
    "print(f\"\\nBefore scaling - Mean: {X.mean().values[:3]}\")\n",
    "print(f\"After scaling - Mean: {X_scaled.mean(axis=0)[:3].round(6)}\")\n",
    "print(f\"After scaling - Std: {X_scaled.std(axis=0)[:3].round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (c): Divide input and output features into five folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"5-Fold Cross Validation Setup:\")\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "    print(f\"  Fold {fold}: Train size = {len(train_idx)}, Test size = {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (d): Run 5 iterations - Find beta matrix, predicted values, and R2 score using Least Square Error Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_fit(X, y):\n",
    "    \"\"\"\n",
    "    Calculate beta coefficients using Least Square Error Fit.\n",
    "    Formula: beta = (X^T * X)^(-1) * X^T * y\n",
    "    \"\"\"\n",
    "    # Add bias term (column of ones)\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    # Calculate beta using normal equation\n",
    "    beta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    return beta\n",
    "\n",
    "def predict_with_beta(X, beta):\n",
    "    \"\"\"Make predictions using beta coefficients.\"\"\"\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b @ beta\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    \"\"\"Calculate R-squared score.\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"Least Square Error Fit functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5-fold cross validation\n",
    "y_values = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"5-FOLD CROSS VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "    # Split data\n",
    "    X_train_fold = X_scaled[train_idx]\n",
    "    X_test_fold = X_scaled[test_idx]\n",
    "    y_train_fold = y_values[train_idx]\n",
    "    y_test_fold = y_values[test_idx]\n",
    "    \n",
    "    # Calculate beta using Least Square Error Fit\n",
    "    beta = least_square_fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_fold = predict_with_beta(X_test_fold, beta)\n",
    "    \n",
    "    # Calculate R2 score\n",
    "    r2 = calculate_r2(y_test_fold, y_pred_fold)\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'beta': beta,\n",
    "        'r2_score': r2,\n",
    "        'y_pred': y_pred_fold\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    print(f\"Beta (Î²) matrix shape: {beta.shape}\")\n",
    "    print(f\"Beta coefficients (first 3): {beta[:3].round(4)}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "# Summary\n",
    "r2_scores = [r['r2_score'] for r in fold_results]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Average R2 Score: {np.mean(r2_scores):.4f}\")\n",
    "print(f\"R2 Scores: {[round(r, 4) for r in r2_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (e): Use best beta matrix for final 70-30 split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best beta (highest R2 score)\n",
    "best_fold = max(fold_results, key=lambda x: x['r2_score'])\n",
    "best_beta = best_fold['beta']\n",
    "best_r2 = best_fold['r2_score']\n",
    "\n",
    "print(f\"Best fold: {best_fold['fold']}\")\n",
    "print(f\"Best R2 score from CV: {best_r2:.4f}\")\n",
    "print(f\"Best Beta coefficients: {best_beta.round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final 70-30 split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_scaled, y_values, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Final Training set: {X_train_final.shape[0]} samples\")\n",
    "print(f\"Final Test set: {X_test_final.shape[0]} samples\")\n",
    "\n",
    "# Train with best beta approach (recalculate on training data)\n",
    "final_beta = least_square_fit(X_train_final, y_train_final)\n",
    "\n",
    "# Test performance\n",
    "y_pred_final = predict_with_beta(X_test_final, final_beta)\n",
    "final_r2 = calculate_r2(y_test_final, y_pred_final)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL MODEL PERFORMANCE (70-30 Split)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Final R2 Score on Test Set: {final_r2:.4f}\")\n",
    "print(f\"Final Beta coefficients: {final_beta.round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: R2 scores across folds\n",
    "folds = [r['fold'] for r in fold_results]\n",
    "axes[0].bar(folds, r2_scores, color='steelblue', edgecolor='black')\n",
    "axes[0].axhline(y=np.mean(r2_scores), color='red', linestyle='--', label=f'Mean R2 = {np.mean(r2_scores):.4f}')\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('R2 Score')\n",
    "axes[0].set_title('R2 Score Across 5 Folds')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Actual vs Predicted\n",
    "axes[1].scatter(y_test_final, y_pred_final, alpha=0.5, color='coral')\n",
    "axes[1].plot([y_test_final.min(), y_test_final.max()], \n",
    "             [y_test_final.min(), y_test_final.max()], 'k--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price')\n",
    "axes[1].set_ylabel('Predicted Price')\n",
    "axes[1].set_title(f'Actual vs Predicted (R2 = {final_r2:.4f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('q1_kfold_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Q2: Validation Set for Multiple Linear Regression (Gradient Descent Optimization)\n",
    "\n",
    "**Dataset**: Same USA Housing dataset\n",
    "\n",
    "**Split**: Training (56%), Validation (14%), Test (30%)\n",
    "\n",
    "**Learning rates**: {0.001, 0.01, 0.1, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Train (56%), Validation (14%), Test (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70% train+val, 30% test\n",
    "X_temp, X_test_q2, y_temp, y_test_q2 = train_test_split(\n",
    "    X_scaled, y_values, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 80% train (56% of total), 20% validation (14% of total)\n",
    "X_train_q2, X_val_q2, y_train_q2, y_val_q2 = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_q2.shape[0]} samples ({X_train_q2.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val_q2.shape[0]} samples ({X_val_q2.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_q2.shape[0]} samples ({X_test_q2.shape[0]/len(X_scaled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_regression(X, y, learning_rate, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Multiple Linear Regression using Gradient Descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input features (scaled)\n",
    "    - y: Target variable\n",
    "    - learning_rate: Learning rate for gradient descent\n",
    "    - n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - theta: Regression coefficients\n",
    "    - cost_history: Cost at each iteration\n",
    "    \"\"\"\n",
    "    # Add bias term\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    m, n = X_b.shape\n",
    "    \n",
    "    # Initialize theta\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Predictions\n",
    "        predictions = X_b @ theta\n",
    "        \n",
    "        # Calculate error\n",
    "        errors = predictions - y\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gradients = (1/m) * (X_b.T @ errors)\n",
    "        \n",
    "        # Update theta\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        # Calculate cost (MSE)\n",
    "        cost = (1/(2*m)) * np.sum(errors**2)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "def predict_gd(X, theta):\n",
    "    \"\"\"Make predictions using gradient descent coefficients.\"\"\"\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b @ theta\n",
    "\n",
    "print(\"Gradient Descent functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with different learning rates and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rates to test\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "n_iterations = 1000\n",
    "\n",
    "results_q2 = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GRADIENT DESCENT WITH DIFFERENT LEARNING RATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n--- Learning Rate: {lr} ---\")\n",
    "    \n",
    "    # Train with gradient descent\n",
    "    theta, cost_history = gradient_descent_regression(X_train_q2, y_train_q2, lr, n_iterations)\n",
    "    \n",
    "    # Predictions on validation and test sets\n",
    "    y_val_pred = predict_gd(X_val_q2, theta)\n",
    "    y_test_pred = predict_gd(X_test_q2, theta)\n",
    "    \n",
    "    # Calculate R2 scores\n",
    "    val_r2 = calculate_r2(y_val_q2, y_val_pred)\n",
    "    test_r2 = calculate_r2(y_test_q2, y_test_pred)\n",
    "    \n",
    "    results_q2.append({\n",
    "        'learning_rate': lr,\n",
    "        'theta': theta,\n",
    "        'cost_history': cost_history,\n",
    "        'val_r2': val_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'final_cost': cost_history[-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Cost: {cost_history[-1]:.4f}\")\n",
    "    print(f\"Validation R2 Score: {val_r2:.4f}\")\n",
    "    print(f\"Test R2 Score: {test_r2:.4f}\")\n",
    "    print(f\"Regression coefficients (first 3): {theta[:3].round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best learning rate based on validation R2\n",
    "valid_results = [r for r in results_q2 if not np.isnan(r['val_r2']) and not np.isinf(r['val_r2'])]\n",
    "\n",
    "if valid_results:\n",
    "    best_result = max(valid_results, key=lambda x: x['val_r2'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BEST LEARNING RATE SELECTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBest Learning Rate: {best_result['learning_rate']}\")\n",
    "    print(f\"Validation R2 Score: {best_result['val_r2']:.4f}\")\n",
    "    print(f\"Test R2 Score: {best_result['test_r2']:.4f}\")\n",
    "    print(f\"Best Regression Coefficients: {best_result['theta'].round(4)}\")\n",
    "else:\n",
    "    print(\"No valid results found. Try adjusting learning rates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Learning Rate': r['learning_rate'],\n",
    "        'Final Cost': r['final_cost'],\n",
    "        'Validation R2': r['val_r2'],\n",
    "        'Test R2': r['test_r2']\n",
    "    } for r in results_q2\n",
    "])\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Cost convergence for different learning rates\n",
    "for r in results_q2:\n",
    "    if r['final_cost'] < 1e10:  # Only plot if converged\n",
    "        axes[0].plot(r['cost_history'][:200], label=f\"LR = {r['learning_rate']}\")\n",
    "\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Cost (MSE)')\n",
    "axes[0].set_title('Cost Convergence for Different Learning Rates')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: R2 scores comparison\n",
    "valid_lr = [r['learning_rate'] for r in valid_results]\n",
    "val_r2s = [r['val_r2'] for r in valid_results]\n",
    "test_r2s = [r['test_r2'] for r in valid_results]\n",
    "\n",
    "x = np.arange(len(valid_lr))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, val_r2s, width, label='Validation R2', color='steelblue')\n",
    "axes[1].bar(x + width/2, test_r2s, width, label='Test R2', color='coral')\n",
    "axes[1].set_xlabel('Learning Rate')\n",
    "axes[1].set_ylabel('R2 Score')\n",
    "axes[1].set_title('R2 Scores for Different Learning Rates')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(valid_lr)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('q2_gradient_descent_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Q3: Pre-processing and Multiple Linear Regression with PCA\n",
    "\n",
    "**Dataset**: Car Price Prediction (imports-85.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load dataset with column names and replace '?' with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names as specified\n",
    "column_names = [\n",
    "    \"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "    \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\", \"wheel_base\",\n",
    "    \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\", \"num_cylinders\",\n",
    "    \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\",\n",
    "    \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df_cars = pd.read_csv('imports-85.csv', header=None, names=column_names, na_values='?')\n",
    "except:\n",
    "    # Try URL\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
    "    df_cars = pd.read_csv(url, header=None, names=column_names, na_values='?')\n",
    "\n",
    "print(f\"Dataset Shape: {df_cars.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df_cars.isnull().sum()[df_cars.isnull().sum() > 0]}\")\n",
    "df_cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Replace NaN with central tendency imputation, drop rows with NaN in price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in price column\n",
    "df_cars = df_cars.dropna(subset=['price'])\n",
    "print(f\"After dropping NaN in price: {df_cars.shape}\")\n",
    "\n",
    "# Fill remaining NaN with central tendency (median for numeric, mode for categorical)\n",
    "for col in df_cars.columns:\n",
    "    if df_cars[col].dtype == 'object':\n",
    "        df_cars[col].fillna(df_cars[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        df_cars[col].fillna(df_cars[col].median(), inplace=True)\n",
    "\n",
    "print(f\"After imputation - Missing values: {df_cars.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert non-numeric values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i) num_doors and num_cylinders: convert words to numbers\n",
    "word_to_num = {\n",
    "    'two': 2, 'three': 3, 'four': 4, 'five': 5, \n",
    "    'six': 6, 'eight': 8, 'twelve': 12\n",
    "}\n",
    "\n",
    "df_cars['num_doors'] = df_cars['num_doors'].replace(word_to_num)\n",
    "df_cars['num_cylinders'] = df_cars['num_cylinders'].replace(word_to_num)\n",
    "print(\"(i) num_doors and num_cylinders converted to numbers\")\n",
    "\n",
    "# (ii) body_style, drive_wheels: dummy encoding\n",
    "df_cars = pd.get_dummies(df_cars, columns=['body_style', 'drive_wheels'], drop_first=True)\n",
    "print(\"(ii) body_style and drive_wheels: dummy encoding applied\")\n",
    "\n",
    "# (iii) make, aspiration, engine_location, fuel_type: label encoding\n",
    "label_cols = ['make', 'aspiration', 'engine_location', 'fuel_type']\n",
    "le = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    df_cars[col] = le.fit_transform(df_cars[col].astype(str))\n",
    "print(\"(iii) make, aspiration, engine_location, fuel_type: label encoding applied\")\n",
    "\n",
    "# (iv) fuel_system: replace 'pfi' containing values to 1, else 0\n",
    "df_cars['fuel_system'] = df_cars['fuel_system'].apply(\n",
    "    lambda x: 1 if 'pfi' in str(x).lower() else 0\n",
    ")\n",
    "print(\"(iv) fuel_system: pfi -> 1, else -> 0\")\n",
    "\n",
    "# (v) engine_type: replace 'ohc' containing values to 1, else 0\n",
    "df_cars['engine_type'] = df_cars['engine_type'].apply(\n",
    "    lambda x: 1 if 'ohc' in str(x).lower() else 0\n",
    ")\n",
    "print(\"(v) engine_type: ohc -> 1, else -> 0\")\n",
    "\n",
    "print(f\"\\nDataset shape after encoding: {df_cars.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Divide into input/output and scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_cars = df_cars.drop('price', axis=1)\n",
    "y_cars = df_cars['price']\n",
    "\n",
    "# Convert all to numeric\n",
    "X_cars = X_cars.apply(pd.to_numeric, errors='coerce')\n",
    "X_cars = X_cars.fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler_cars = StandardScaler()\n",
    "X_cars_scaled = scaler_cars.fit_transform(X_cars)\n",
    "\n",
    "print(f\"Features shape: {X_cars_scaled.shape}\")\n",
    "print(f\"Target shape: {y_cars.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train linear regressor on 70% data, test on 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_cars, X_test_cars, y_train_cars, y_test_cars = train_test_split(\n",
    "    X_cars_scaled, y_cars, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_cars.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_cars.shape[0]} samples\")\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_cars, y_train_cars)\n",
    "\n",
    "# Predictions\n",
    "y_pred_cars = lr_model.predict(X_test_cars)\n",
    "\n",
    "# Evaluate\n",
    "r2_no_pca = r2_score(y_test_cars, y_pred_cars)\n",
    "rmse_no_pca = np.sqrt(mean_squared_error(y_test_cars, y_pred_cars))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LINEAR REGRESSION WITHOUT PCA\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"R2 Score: {r2_no_pca:.4f}\")\n",
    "print(f\"RMSE: {rmse_no_pca:.2f}\")\n",
    "print(f\"Number of features: {X_train_cars.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply PCA and retrain linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_cars)\n",
    "X_test_pca = pca.transform(X_test_cars)\n",
    "\n",
    "print(f\"Original features: {X_train_cars.shape[1]}\")\n",
    "print(f\"Features after PCA: {X_train_pca.shape[1]}\")\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression on PCA-reduced data\n",
    "lr_model_pca = LinearRegression()\n",
    "lr_model_pca.fit(X_train_pca, y_train_cars)\n",
    "\n",
    "# Predictions\n",
    "y_pred_pca = lr_model_pca.predict(X_test_pca)\n",
    "\n",
    "# Evaluate\n",
    "r2_with_pca = r2_score(y_test_cars, y_pred_pca)\n",
    "rmse_with_pca = np.sqrt(mean_squared_error(y_test_cars, y_pred_pca))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LINEAR REGRESSION WITH PCA\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"R2 Score: {r2_with_pca:.4f}\")\n",
    "print(f\"RMSE: {rmse_with_pca:.2f}\")\n",
    "print(f\"Number of features: {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON: WITH vs WITHOUT PCA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "comparison_pca = pd.DataFrame({\n",
    "    'Model': ['Without PCA', 'With PCA'],\n",
    "    'Features': [X_train_cars.shape[1], X_train_pca.shape[1]],\n",
    "    'R2 Score': [r2_no_pca, r2_with_pca],\n",
    "    'RMSE': [rmse_no_pca, rmse_with_pca]\n",
    "})\n",
    "print(comparison_pca.to_string(index=False))\n",
    "\n",
    "if r2_with_pca > r2_no_pca:\n",
    "    print(f\"\\nConclusion: Yes, PCA led to performance improvement! (+{(r2_with_pca-r2_no_pca)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\nConclusion: PCA did not improve R2, but reduced features from {X_train_cars.shape[1]} to {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot 1: R2 Comparison\n",
    "models = ['Without PCA', 'With PCA']\n",
    "r2_scores = [r2_no_pca, r2_with_pca]\n",
    "colors = ['steelblue', 'coral']\n",
    "axes[0].bar(models, r2_scores, color=colors)\n",
    "axes[0].set_ylabel('R2 Score')\n",
    "axes[0].set_title('R2 Score Comparison')\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Explained Variance by PCA components\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, len(cum_var)+1), cum_var, 'bo-')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('PCA Explained Variance')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Actual vs Predicted (With PCA)\n",
    "axes[2].scatter(y_test_cars, y_pred_pca, alpha=0.5, color='coral')\n",
    "axes[2].plot([y_test_cars.min(), y_test_cars.max()], \n",
    "             [y_test_cars.min(), y_test_cars.max()], 'k--', lw=2)\n",
    "axes[2].set_xlabel('Actual Price')\n",
    "axes[2].set_ylabel('Predicted Price')\n",
    "axes[2].set_title(f'Actual vs Predicted (PCA, R2={r2_with_pca:.4f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('q3_pca_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "- **Q1**: Implemented 5-Fold Cross Validation for Multiple Linear Regression using Least Square Error Fit\n",
    "- **Q2**: Used Gradient Descent Optimization with different learning rates and validation set approach\n",
    "- **Q3**: Applied data preprocessing, encoding, and PCA for dimensionality reduction on car price prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
