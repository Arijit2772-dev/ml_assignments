{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2024.12.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Solving Question 1: Scraping 4 Book Categories ---\n",
      "Scraping category: Travel...\n",
      "Scraping category: Mystery...\n",
      "Scraping category: Historical Fiction...\n",
      "Scraping category: Sequential Art...\n",
      "\n",
      "Scraping complete. Data saved to 'books_by_category.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Solving Question 1: Scraping 4 Book Categories ---\")\n",
    "\n",
    "# Base URL for the website\n",
    "base_url = \"https://books.toscrape.com/\"\n",
    "all_books_data = []\n",
    "\n",
    "# The specific category pages to scrape\n",
    "categories = [\n",
    "    'catalogue/category/books/travel_2/index.html',\n",
    "    'catalogue/category/books/mystery_3/index.html',\n",
    "    'catalogue/category/books/historical-fiction_4/index.html',\n",
    "    'catalogue/category/books/sequential-art_5/index.html'\n",
    "]\n",
    "\n",
    "for category_path in categories:\n",
    "    url = base_url + category_path\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    category_name = soup.find('h1').text\n",
    "    print(f\"Scraping category: {category_name}...\")\n",
    "    \n",
    "    # Find all the book containers on the page\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        title = book.find('h3').find('a')['title']\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        stock = book.find('p', class_='instock availability').text.strip()\n",
    "        # The star rating is in the class name, e.g., \"star-rating Three\"\n",
    "        rating = book.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "        all_books_data.append([title, price, stock, rating, category_name])\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(all_books_data, columns=['Title', 'Price', 'Stock', 'Rating', 'Category'])\n",
    "df.to_csv('books_by_category.csv', index=False)\n",
    "\n",
    "print(\"\\nScraping complete. Data saved to 'books_by_category.csv'.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Solving Question 2: Scraping IMDb's Top 250 Movies ---\n",
      "\n",
      "Scraping complete. Found 25 movies.\n",
      "Data saved to 'imdb_top250_movies.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"--- Solving Question 2: Scraping IMDb's Top 250 Movies ---\")\n",
    "\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "movies_data = []\n",
    "\n",
    "movie_list_items = soup.select(\".ipc-metadata-list-summary-item\")\n",
    "\n",
    "# Loop through ALL the items in the list we found (no [:50] slice).\n",
    "for item in movie_list_items:\n",
    "    # Find the <h3> tag which contains the movie's rank and title.\n",
    "    title_tag = item.find(\"h3\", class_=\"ipc-title__text\")\n",
    "    # The text looks like \"1. The Shawshank Redemption\". We split it at the dot and take the second part.\n",
    "    name = title_tag.text.split(\".\")[1].strip()\n",
    "\n",
    "    # The metadata div contains the year, duration, and rating.\n",
    "    metadata_div = item.find(\"div\", class_=\"cli-title-metadata\")\n",
    "    metadata_items = metadata_div.find_all(\"span\", class_=\"cli-title-metadata-item\")\n",
    "    # The year is in the first span.\n",
    "    year = metadata_items[0].text.strip()\n",
    "\n",
    "    # The rating is in a specific div and span structure.\n",
    "    rating_span = item.find(\"span\", class_=\"ipc-rating-star\")\n",
    "    # The text inside looks like \"9.3 (3.0M)\". We split by space and take the first part.\n",
    "    rating = rating_span.text.split(\" \")[0]\n",
    "\n",
    "    movies_data.append([name, year, rating])\n",
    "\n",
    "df = pd.DataFrame(movies_data, columns=[\"Name\", \"Year\", \"Rating\"])\n",
    "df.to_csv(\"imdb_top250_movies.csv\", index=False)\n",
    "\n",
    "print(f\"\\nScraping complete. Found {len(movies_data)} movies.\")\n",
    "print(\"Data saved to 'imdb_top250_movies.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.6.15 (from selenium)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing_extensions~=4.14.0 (from selenium)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, typing_extensions, pysocks, outcome, certifi, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.12.14\n",
      "    Uninstalling certifi-2024.12.14:\n",
      "      Successfully uninstalled certifi-2024.12.14\n",
      "Successfully installed certifi-2025.8.3 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.35.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from webdriver-manager) (2.32.4)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (2025.8.3)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.1.1 webdriver-manager-4.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Solving Question 2 (Corrected): Scraping All 250 IMDb Movies ---\n",
      "Browser opened. Waiting for page to load and scrolling...\n",
      "Scrolling complete and browser closed.\n",
      "\n",
      "Scraping complete. Found 250 movies.\n",
      "Data saved to 'imdb_top250_movies.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"--- Solving Question 2 (Corrected): Scraping All 250 IMDb Movies ---\")\n",
    "\n",
    "# --- Part 1: Control the Browser with Selenium ---\n",
    "\n",
    "# Setup and launch a Chrome browser automatically\n",
    "# This will open a new Chrome window\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Go to the IMDb Top 250 URL\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "driver.get(url)\n",
    "print(\"Browser opened. Waiting for page to load and scrolling...\")\n",
    "\n",
    "# Scroll down the page multiple times to load all movies\n",
    "# We give it a few seconds to make sure all content loads\n",
    "scroll_pause_time = 2\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Get the complete HTML source code after everything has loaded\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "print(\"Scrolling complete and browser closed.\")\n",
    "\n",
    "# --- Part 2: Parse the Complete HTML with BeautifulSoup ---\n",
    "\n",
    "# Turn the complete HTML into a searchable BeautifulSoup object\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "movies_data = []\n",
    "movie_list_items = soup.select(\".ipc-metadata-list-summary-item\")\n",
    "\n",
    "# Loop through ALL the items in the list we found\n",
    "for item in movie_list_items:\n",
    "    title_tag = item.find(\"h3\", class_=\"ipc-title__text\")\n",
    "    name = title_tag.text.split(\".\")[1].strip()\n",
    "\n",
    "    metadata_div = item.find(\"div\", class_=\"cli-title-metadata\")\n",
    "    metadata_items = metadata_div.find_all(\"span\", class_=\"cli-title-metadata-item\")\n",
    "    year = metadata_items[0].text.strip()\n",
    "\n",
    "    rating_span = item.find(\"span\", class_=\"ipc-rating-star\")\n",
    "    rating = rating_span.text.split(\" \")[0]\n",
    "\n",
    "    movies_data.append([name, year, rating])\n",
    "\n",
    "df = pd.DataFrame(movies_data, columns=[\"Name\", \"Year\", \"Rating\"])\n",
    "df.to_csv(\"imdb_top250_movies.csv\", index=False)\n",
    "\n",
    "print(f\"\\nScraping complete. Found {len(movies_data)} movies.\")\n",
    "print(\"Data saved to 'imdb_top250_movies.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Solving Question 3 (Final Corrected Version): Scraping Weather Data ---\n",
      "\n",
      "Scraping complete. Data saved to 'world_weather.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Solving Question 3 (Final Corrected Version): Scraping Weather Data ---\")\n",
    "\n",
    "url = \"https://www.timeanddate.com/weather/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "weather_data = []\n",
    "\n",
    "weather_table = soup.find(\"table\", class_=\"zebra fw tb-theme\")\n",
    "city_rows = weather_table.find_all(\"tr\")\n",
    "\n",
    "# Loop through the first 11 rows, skipping the header with [1:11]\n",
    "for row in city_rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    \n",
    "    # Extract data that is always in the same place\n",
    "    city = cells[0].text.strip()\n",
    "    temperature = cells[2].text.strip()\n",
    "    wind = cells[5].text.strip()\n",
    "    \n",
    "    # --- NEW LOGIC to handle the weather description ---\n",
    "    # Get the cell where the weather description is located.\n",
    "    weather_cell = cells[1]\n",
    "    # First, look for an <img> tag inside this cell.\n",
    "    img_tag = weather_cell.find('img')\n",
    "    \n",
    "    # IF an image tag exists, get the weather from its 'title'.\n",
    "    if img_tag:\n",
    "        weather = img_tag['title']\n",
    "    # ELSE (if no image tag exists), get the weather from the cell's plain text.\n",
    "    else:\n",
    "        weather = weather_cell.text.strip()\n",
    "\n",
    "    # Add the correctly found data to our list.\n",
    "    weather_data.append([city, weather, temperature, wind])\n",
    "\n",
    "df = pd.DataFrame(weather_data, columns=[\"City\", \"Weather\", \"Temperature\", \"Wind Speed\"])\n",
    "df.to_csv(\"world_weather.csv\", index=False)\n",
    "\n",
    "print(\"\\nScraping complete. Data saved to 'world_weather.csv'.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
