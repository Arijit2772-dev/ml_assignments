{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - Naive Bayes & KNN Solutions\n",
    "This notebook contains solutions for all questions in Lab Assignment 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Gaussian Naive Bayes Classifier on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Step-by-Step Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayesFromScratch:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implemented from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.priors = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the classifier by calculating mean, variance, and priors for each class.\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.priors[c] = X_c.shape[0] / n_samples\n",
    "            self.mean[c] = np.mean(X_c, axis=0)\n",
    "            self.var[c] = np.var(X_c, axis=0)\n",
    "        \n",
    "        print(\"Model trained successfully!\")\n",
    "        print(f\"Classes: {self.classes}\")\n",
    "        print(f\"Priors: {self.priors}\")\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        \"\"\"Calculate Gaussian likelihood.\"\"\"\n",
    "        eps = 1e-6\n",
    "        coefficient = 1.0 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = np.exp(-((x - mean) ** 2) / (2 * var + eps))\n",
    "        return coefficient * exponent\n",
    "    \n",
    "    def _calculate_posterior(self, x):\n",
    "        \"\"\"Calculate posterior probability for each class.\"\"\"\n",
    "        posteriors = {}\n",
    "        for c in self.classes:\n",
    "            posterior = np.log(self.priors[c])\n",
    "            for i in range(len(x)):\n",
    "                likelihood = self._calculate_likelihood(x[i], self.mean[c][i], self.var[c][i])\n",
    "                posterior += np.log(likelihood + 1e-10)\n",
    "            posteriors[c] = posterior\n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            predicted_class = max(posteriors, key=posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test custom implementation\n",
    "print(\"=\"*70)\n",
    "print(\"PART (i): STEP-BY-STEP IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gnb_custom = GaussianNaiveBayesFromScratch()\n",
    "gnb_custom.fit(X_train, y_train)\n",
    "y_pred_custom = gnb_custom.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for custom implementation\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_custom):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_custom, target_names=iris.target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (ii): Using Built-in Sklearn Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PART (ii): USING BUILT-IN FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Built-in GaussianNB\n",
    "gnb_sklearn = GaussianNB()\n",
    "gnb_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = gnb_sklearn.predict(X_test)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_sklearn):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_sklearn, target_names=iris.target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both implementations\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Custom Implementation Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}\")\n",
    "print(f\"Sklearn Implementation Accuracy: {accuracy_score(y_test, y_pred_sklearn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: GridSearchCV for K-NN Classifier Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GRIDSEARCHCV FOR K-NN CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: Iris\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for K values\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),  # K values from 1 to 30\n",
    "    'weights': ['uniform', 'distance'],  # Weight functions\n",
    "    'metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "print(\"\\nParameter Grid:\")\n",
    "print(f\"  n_neighbors: 1 to 30\")\n",
    "print(f\"  weights: {param_grid['weights']}\")\n",
    "print(f\"  metric: {param_grid['metric']}\")\n",
    "print(f\"\\nTotal combinations: {len(param_grid['n_neighbors']) * len(param_grid['weights']) * len(param_grid['metric'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV\n",
    "print(\"\\nPerforming GridSearchCV with 5-fold cross-validation...\")\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRIDSEARCHCV RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Best K value: {grid_search.best_params_['n_neighbors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 parameter combinations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 PARAMETER COMBINATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = results_df.nlargest(10, 'mean_test_score')[['param_n_neighbors', 'param_weights', 'param_metric', 'mean_test_score', 'std_test_score']]\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: K value vs Accuracy for different weights (euclidean)\n",
    "euclidean_uniform = results_df[(results_df['param_metric'] == 'euclidean') & (results_df['param_weights'] == 'uniform')]\n",
    "euclidean_distance = results_df[(results_df['param_metric'] == 'euclidean') & (results_df['param_weights'] == 'distance')]\n",
    "\n",
    "axes[0, 0].plot(euclidean_uniform['param_n_neighbors'], euclidean_uniform['mean_test_score'], \n",
    "                marker='o', label='Uniform', linewidth=2)\n",
    "axes[0, 0].plot(euclidean_distance['param_n_neighbors'], euclidean_distance['mean_test_score'], \n",
    "                marker='s', label='Distance', linewidth=2)\n",
    "axes[0, 0].axvline(grid_search.best_params_['n_neighbors'], color='red', \n",
    "                   linestyle='--', label=f\"Best K = {grid_search.best_params_['n_neighbors']}\")\n",
    "axes[0, 0].set_xlabel('K (Number of Neighbors)')\n",
    "axes[0, 0].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[0, 0].set_title('K vs Accuracy (Euclidean Distance)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: K value vs Accuracy for different metrics (uniform)\n",
    "uniform_euclidean = results_df[(results_df['param_weights'] == 'uniform') & (results_df['param_metric'] == 'euclidean')]\n",
    "uniform_manhattan = results_df[(results_df['param_weights'] == 'uniform') & (results_df['param_metric'] == 'manhattan')]\n",
    "\n",
    "axes[0, 1].plot(uniform_euclidean['param_n_neighbors'], uniform_euclidean['mean_test_score'], \n",
    "                marker='o', label='Euclidean', linewidth=2)\n",
    "axes[0, 1].plot(uniform_manhattan['param_n_neighbors'], uniform_manhattan['mean_test_score'], \n",
    "                marker='s', label='Manhattan', linewidth=2)\n",
    "axes[0, 1].set_xlabel('K (Number of Neighbors)')\n",
    "axes[0, 1].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[0, 1].set_title('K vs Accuracy (Uniform Weights)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Heatmap of mean scores\n",
    "pivot_table = results_df[results_df['param_metric'] == 'euclidean'].pivot_table(\n",
    "    values='mean_test_score', \n",
    "    index='param_weights', \n",
    "    columns='param_n_neighbors'\n",
    ")\n",
    "sns.heatmap(pivot_table, annot=False, cmap='YlGnBu', ax=axes[1, 0], cbar_kws={'label': 'Accuracy'})\n",
    "axes[1, 0].set_title('Accuracy Heatmap (Euclidean Distance)')\n",
    "axes[1, 0].set_xlabel('K (Number of Neighbors)')\n",
    "axes[1, 0].set_ylabel('Weight Function')\n",
    "\n",
    "# Plot 4: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names, ax=axes[1, 1])\n",
    "axes[1, 1].set_title(f'Confusion Matrix (Best Model: K={grid_search.best_params_[\"n_neighbors\"]})')\n",
    "axes[1, 1].set_ylabel('True Label')\n",
    "axes[1, 1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gridsearch_knn_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "- **Q1**: Implemented Gaussian Naive Bayes from scratch and compared with sklearn\n",
    "- **Q2**: Used GridSearchCV to find optimal K-NN hyperparameters (n_neighbors, weights, metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
