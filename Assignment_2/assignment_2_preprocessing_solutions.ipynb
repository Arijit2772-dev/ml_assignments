{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Data Preprocessing & Feature Engineering Solutions\n",
    "\n",
    "This notebook contains complete solutions for all parts of Assignment 2.\n",
    "\n",
    "**Dataset**: Microsoft Adventure Works Cycles Customer Data\n",
    "\n",
    "**Objective**: Predict which customers are most likely to purchase a bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Feature Selection, Cleaning, and Preprocessing\n",
    "\n",
    "Construct an input dataset from the data source by selecting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer and sales data\n",
    "df_customers = pd.read_csv(\"AWCustomers.csv\")\n",
    "df_sales = pd.read_csv(\"AWSales.csv\")\n",
    "\n",
    "print(f\"Customers Dataset Shape: {df_customers.shape}\")\n",
    "print(f\"Sales Dataset Shape: {df_sales.shape}\")\n",
    "\n",
    "print(\"\\nCustomers Columns:\")\n",
    "print(df_customers.columns.tolist())\n",
    "\n",
    "print(\"\\nSales Columns:\")\n",
    "print(df_sales.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"Customers Data Preview:\")\n",
    "df_customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "df_sales_clean = df_sales.drop(['CustomerID'], axis=1)\n",
    "df = pd.concat([df_customers, df_sales_clean], axis=1)\n",
    "\n",
    "print(f\"Merged Dataset Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I(a): Examine and Select Relevant Attributes\n",
    "\n",
    "Analyze each attribute and select only those that would affect predicting future bike buyers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine all columns\n",
    "print(\"=\"*60)\n",
    "print(\"ATTRIBUTE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAll columns in merged dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1}. {col} - dtype: {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unnecessary attributes that won't help predict bike buyers\n",
    "unnecessary_cols = [\n",
    "    'Title',           # Mostly null, not predictive\n",
    "    'Suffix',          # Mostly null, not predictive\n",
    "    'MiddleName',      # Not predictive for buying behavior\n",
    "    'AddressLine2',    # Mostly null\n",
    "    'PhoneNumber',     # Personal info, not predictive\n",
    "    'FirstName',       # Personal identifier, not predictive\n",
    "    'LastName',        # Personal identifier, not predictive\n",
    "    'AddressLine1',    # Too specific, not generalizable\n",
    "    'City',            # Too many unique values\n",
    "    'PostalCode',      # Too specific\n",
    "    'LastUpdated'      # System metadata, not predictive\n",
    "]\n",
    "\n",
    "# Remove unnecessary attributes\n",
    "df_selected = df.drop(columns=[col for col in unnecessary_cols if col in df.columns])\n",
    "\n",
    "print(\"Removed attributes:\")\n",
    "for col in unnecessary_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nRemaining attributes: {df_selected.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I(b): Create New DataFrame with Selected Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with selected features\n",
    "print(\"=\"*60)\n",
    "print(\"SELECTED FEATURES FOR BIKE BUYER PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nSelected columns ({len(df_selected.columns)}):\")\n",
    "for col in df_selected.columns:\n",
    "    null_count = df_selected[col].isnull().sum()\n",
    "    print(f\"  - {col}: {df_selected[col].dtype}, nulls: {null_count}\")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_selected.shape}\")\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I(c): Determine Data Value Type for Each Attribute\n",
    "\n",
    "Classify each attribute as:\n",
    "- **Discrete** or **Continuous**\n",
    "- **Nominal**, **Ordinal**, **Interval**, or **Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify data types\n",
    "print(\"=\"*70)\n",
    "print(\"DATA TYPE CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Discrete vs Continuous\n",
    "discrete_vars = []\n",
    "continuous_vars = []\n",
    "\n",
    "for col in df_selected.columns:\n",
    "    if df_selected[col].dtype in ['int64', 'object', 'bool']:\n",
    "        discrete_vars.append(col)\n",
    "    else:\n",
    "        continuous_vars.append(col)\n",
    "\n",
    "print(\"\\n1. DISCRETE vs CONTINUOUS:\")\n",
    "print(f\"   Discrete Variables: {discrete_vars}\")\n",
    "print(f\"   Continuous Variables: {continuous_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal, Ordinal, Interval, Ratio classification\n",
    "nominal_vars = [\n",
    "    'CustomerID', 'Gender', 'MaritalStatus', 'StateProvinceName', \n",
    "    'CountryRegionName', 'Education', 'Occupation', 'HomeOwnerFlag', 'BikeBuyer'\n",
    "]\n",
    "\n",
    "ordinal_vars = [\n",
    "    'Education'  # Has inherent ordering (High School < Bachelors < Graduate)\n",
    "]\n",
    "\n",
    "interval_vars = [\n",
    "    'BirthDate'  # Dates are interval scale\n",
    "]\n",
    "\n",
    "ratio_vars = [\n",
    "    'NumberCarsOwned', 'NumberChildrenAtHome', 'TotalChildren', \n",
    "    'YearlyIncome', 'AvgMonthSpend'\n",
    "]\n",
    "\n",
    "print(\"\\n2. NOMINAL / ORDINAL / INTERVAL / RATIO:\")\n",
    "print(f\"   Nominal (categorical, no order): {[v for v in nominal_vars if v in df_selected.columns]}\")\n",
    "print(f\"   Ordinal (categorical, ordered): {[v for v in ordinal_vars if v in df_selected.columns]}\")\n",
    "print(f\"   Interval (numeric, no true zero): {[v for v in interval_vars if v in df_selected.columns]}\")\n",
    "print(f\"   Ratio (numeric, true zero exists): {[v for v in ratio_vars if v in df_selected.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "type_summary = []\n",
    "for col in df_selected.columns:\n",
    "    disc_cont = 'Discrete' if col in discrete_vars else 'Continuous'\n",
    "    if col in nominal_vars:\n",
    "        scale = 'Nominal'\n",
    "    elif col in ordinal_vars:\n",
    "        scale = 'Ordinal'\n",
    "    elif col in interval_vars:\n",
    "        scale = 'Interval'\n",
    "    else:\n",
    "        scale = 'Ratio'\n",
    "    type_summary.append({'Attribute': col, 'Discrete/Continuous': disc_cont, 'Scale': scale})\n",
    "\n",
    "type_df = pd.DataFrame(type_summary)\n",
    "print(\"\\nCOMPLETE DATA TYPE SUMMARY:\")\n",
    "print(type_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Data Preprocessing and Transformation\n",
    "\n",
    "Transform the preprocessed data based on data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II(a): Handling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HANDLING NULL VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check null values before\n",
    "print(\"\\nNull values BEFORE imputation:\")\n",
    "null_counts = df_selected.isnull().sum()\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['YearlyIncome', 'AvgMonthSpend', 'NumberCarsOwned', \n",
    "                      'NumberChildrenAtHome', 'TotalChildren']\n",
    "categorical_features = ['Gender', 'MaritalStatus', 'HomeOwnerFlag', 'BikeBuyer']\n",
    "\n",
    "# Impute numerical features with mean\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "for col in numerical_features:\n",
    "    if col in df_selected.columns:\n",
    "        df_selected[col] = numerical_imputer.fit_transform(df_selected[[col]])\n",
    "\n",
    "# Impute categorical features with mode\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "for col in categorical_features:\n",
    "    if col in df_selected.columns:\n",
    "        df_selected[col] = categorical_imputer.fit_transform(df_selected[[col]]).ravel()\n",
    "\n",
    "print(\"\\nNull values AFTER imputation:\")\n",
    "print(f\"Total nulls remaining: {df_selected.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II(b): Normalization (Min-Max Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NORMALIZATION (Min-Max Scaling)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for normalized data\n",
    "df_normalized = df_selected.copy()\n",
    "\n",
    "# Apply Min-Max normalization to numerical features\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "print(\"\\nBEFORE Normalization:\")\n",
    "print(df_normalized[numerical_features].describe().loc[['min', 'max']].T)\n",
    "\n",
    "df_normalized[numerical_features] = min_max_scaler.fit_transform(df_normalized[numerical_features])\n",
    "\n",
    "print(\"\\nAFTER Normalization (values scaled to 0-1):\")\n",
    "print(df_normalized[numerical_features].describe().loc[['min', 'max']].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II(c): Discretization (Binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DISCRETIZATION (Binning)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply binning to YearlyIncome (continuous attribute with many values)\n",
    "binning_transformer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Get original YearlyIncome before normalization\n",
    "original_income = df_selected['YearlyIncome'].copy()\n",
    "\n",
    "df_normalized['YearlyIncome_Binned'] = binning_transformer.fit_transform(\n",
    "    df_selected[['YearlyIncome']]\n",
    ").astype(int)\n",
    "\n",
    "print(\"\\nYearlyIncome Binning (5 bins using quantiles):\")\n",
    "print(df_normalized['YearlyIncome_Binned'].value_counts().sort_index())\n",
    "\n",
    "# Show bin edges\n",
    "bin_edges = binning_transformer.bin_edges_[0]\n",
    "print(f\"\\nBin edges: {bin_edges.round(0)}\")\n",
    "print(\"\\nBin labels: 0=Low, 1=Below-Average, 2=Average, 3=Above-Average, 4=High\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II(d): Standardization (Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STANDARDIZATION (Z-Score)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply StandardScaler (mean=0, std=1)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "df_standardized = df_selected.copy()\n",
    "df_standardized[numerical_features] = standard_scaler.fit_transform(\n",
    "    df_selected[numerical_features]\n",
    ")\n",
    "\n",
    "print(\"\\nAFTER Standardization (mean=0, std=1):\")\n",
    "stats = df_standardized[numerical_features].describe().loc[['mean', 'std']].T\n",
    "print(stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II(e): Binarization (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BINARIZATION (One-Hot Encoding)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Columns to one-hot encode\n",
    "cols_to_encode = ['Gender', 'MaritalStatus']\n",
    "\n",
    "print(\"\\nBEFORE One-Hot Encoding:\")\n",
    "for col in cols_to_encode:\n",
    "    if col in df_standardized.columns:\n",
    "        print(f\"  {col}: {df_standardized[col].unique()}\")\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df_standardized, columns=cols_to_encode, drop_first=False)\n",
    "\n",
    "print(\"\\nAFTER One-Hot Encoding:\")\n",
    "new_cols = [c for c in df_encoded.columns if any(orig in c for orig in cols_to_encode)]\n",
    "print(f\"  New columns created: {new_cols}\")\n",
    "\n",
    "print(f\"\\nDataset shape after encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of encoded data\n",
    "print(\"\\nSample of One-Hot Encoded Data:\")\n",
    "df_encoded[new_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part III: Calculating Proximity / Correlation Analysis\n",
    "\n",
    "Calculate similarity measures between objects and correlation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III(a): Calculate Similarity Measures\n",
    "\n",
    "Calculate **Simple Matching Coefficient**, **Jaccard Similarity**, and **Cosine Similarity** between two objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SIMILARITY MEASURES BETWEEN TWO OBJECTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select two sample objects (rows) for comparison\n",
    "# Use binary/categorical columns for similarity calculation\n",
    "binary_cols = [c for c in df_encoded.columns if df_encoded[c].nunique() <= 2]\n",
    "\n",
    "# Get two sample objects\n",
    "obj1 = df_encoded[binary_cols].iloc[0].values\n",
    "obj2 = df_encoded[binary_cols].iloc[1].values\n",
    "\n",
    "print(f\"\\nComparing Object 1 (Row 0) and Object 2 (Row 1)\")\n",
    "print(f\"Binary features used: {len(binary_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_matching_coefficient(a, b):\n",
    "    \"\"\"\n",
    "    Simple Matching Coefficient (SMC)\n",
    "    SMC = (f11 + f00) / (f11 + f10 + f01 + f00)\n",
    "    Where:\n",
    "    - f11: both 1\n",
    "    - f00: both 0\n",
    "    - f10: a=1, b=0\n",
    "    - f01: a=0, b=1\n",
    "    \"\"\"\n",
    "    a = np.array(a).astype(int)\n",
    "    b = np.array(b).astype(int)\n",
    "    \n",
    "    f11 = np.sum((a == 1) & (b == 1))\n",
    "    f00 = np.sum((a == 0) & (b == 0))\n",
    "    f10 = np.sum((a == 1) & (b == 0))\n",
    "    f01 = np.sum((a == 0) & (b == 1))\n",
    "    \n",
    "    smc = (f11 + f00) / (f11 + f10 + f01 + f00)\n",
    "    return smc, f11, f00, f10, f01\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity\n",
    "    J = f11 / (f11 + f10 + f01)\n",
    "    Ignores cases where both are 0\n",
    "    \"\"\"\n",
    "    a = np.array(a).astype(int)\n",
    "    b = np.array(b).astype(int)\n",
    "    \n",
    "    f11 = np.sum((a == 1) & (b == 1))\n",
    "    f10 = np.sum((a == 1) & (b == 0))\n",
    "    f01 = np.sum((a == 0) & (b == 1))\n",
    "    \n",
    "    if (f11 + f10 + f01) == 0:\n",
    "        return 0\n",
    "    return f11 / (f11 + f10 + f01)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Cosine Similarity\n",
    "    cos(a,b) = (a·b) / (||a|| * ||b||)\n",
    "    \"\"\"\n",
    "    a = np.array(a).astype(float)\n",
    "    b = np.array(b).astype(float)\n",
    "    \n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(\"Similarity functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all similarity measures\n",
    "print(\"=\"*60)\n",
    "print(\"SIMILARITY CALCULATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Simple Matching Coefficient\n",
    "smc, f11, f00, f10, f01 = simple_matching_coefficient(obj1, obj2)\n",
    "print(f\"\\n1. SIMPLE MATCHING COEFFICIENT (SMC):\")\n",
    "print(f\"   f11 (both 1): {f11}\")\n",
    "print(f\"   f00 (both 0): {f00}\")\n",
    "print(f\"   f10 (a=1, b=0): {f10}\")\n",
    "print(f\"   f01 (a=0, b=1): {f01}\")\n",
    "print(f\"   SMC = (f11 + f00) / total = ({f11} + {f00}) / {f11+f00+f10+f01}\")\n",
    "print(f\"   SMC = {smc:.4f}\")\n",
    "\n",
    "# 2. Jaccard Similarity\n",
    "jaccard = jaccard_similarity(obj1, obj2)\n",
    "print(f\"\\n2. JACCARD SIMILARITY:\")\n",
    "print(f\"   J = f11 / (f11 + f10 + f01) = {f11} / ({f11} + {f10} + {f01})\")\n",
    "print(f\"   Jaccard Similarity = {jaccard:.4f}\")\n",
    "\n",
    "# 3. Cosine Similarity\n",
    "cos_sim = cosine_sim(obj1, obj2)\n",
    "print(f\"\\n3. COSINE SIMILARITY:\")\n",
    "print(f\"   cos(a,b) = (a·b) / (||a|| * ||b||)\")\n",
    "print(f\"   Cosine Similarity = {cos_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMILARITY MEASURES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'Measure': ['Simple Matching Coefficient', 'Jaccard Similarity', 'Cosine Similarity'],\n",
    "    'Value': [smc, jaccard, cos_sim],\n",
    "    'Interpretation': [\n",
    "        'Considers both matches (1-1 and 0-0)',\n",
    "        'Only considers positive matches (1-1)',\n",
    "        'Measures angle between vectors'\n",
    "    ]\n",
    "})\n",
    "print(similarity_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III(b): Calculate Correlation Between Features\n",
    "\n",
    "Calculate correlation between **Commute Distance** and **Yearly Income**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if CommuteDistance exists, if not use available numeric columns\n",
    "if 'CommuteDistance' in df_selected.columns:\n",
    "    feature1 = 'CommuteDistance'\n",
    "else:\n",
    "    feature1 = 'NumberCarsOwned'  # Alternative feature\n",
    "\n",
    "feature2 = 'YearlyIncome'\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation = df_selected[feature1].corr(df_selected[feature2])\n",
    "\n",
    "print(f\"\\nCorrelation between {feature1} and {feature2}:\")\n",
    "print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if abs(correlation) < 0.3:\n",
    "    strength = \"Weak\"\n",
    "elif abs(correlation) < 0.7:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Strong\"\n",
    "\n",
    "direction = \"positive\" if correlation > 0 else \"negative\"\n",
    "\n",
    "print(f\"\\nInterpretation: {strength} {direction} correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all numerical features\n",
    "print(\"\\nFull Correlation Matrix (Numerical Features):\")\n",
    "corr_matrix = df_selected[numerical_features].corr()\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.3f', square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "This notebook completed all parts of Assignment 2:\n",
    "\n",
    "**Part I - Feature Selection & Preprocessing:**\n",
    "- (a) Examined attributes and removed unnecessary ones\n",
    "- (b) Created new DataFrame with selected features\n",
    "- (c) Classified data types (Discrete/Continuous, Nominal/Ordinal/Interval/Ratio)\n",
    "\n",
    "**Part II - Data Transformation:**\n",
    "- (a) Handled null values using mean/mode imputation\n",
    "- (b) Applied Min-Max normalization\n",
    "- (c) Applied discretization (binning) to continuous features\n",
    "- (d) Applied standardization (Z-score normalization)\n",
    "- (e) Applied one-hot encoding to categorical features\n",
    "\n",
    "**Part III - Similarity & Correlation:**\n",
    "- (a) Calculated Simple Matching, Jaccard, and Cosine similarity between objects\n",
    "- (b) Calculated correlation between numerical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
