{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Web Scraping Solutions\n",
    "\n",
    "This notebook contains solutions for all questions in Assignment 4.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Web scraping with BeautifulSoup and Requests\n",
    "- Dynamic content scraping with Selenium\n",
    "- Data extraction and CSV export\n",
    "- Handling pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install requests beautifulsoup4 pandas selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Scrape Books from Books to Scrape\n",
    "\n",
    "**Website**: https://books.toscrape.com/\n",
    "\n",
    "**Task**: Scrape all available books with:\n",
    "1. Title\n",
    "2. Price\n",
    "3. Availability (In stock / Out of stock)\n",
    "4. Star Rating\n",
    "\n",
    "**Note**: Handle pagination to scrape books from ALL pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the Website Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the website\n",
    "base_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Test connection\n",
    "response = requests.get(base_url)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Content Length: {len(response.content)} bytes\")\n",
    "\n",
    "# Parse initial page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find total pages\n",
    "pager = soup.find('li', class_='current')\n",
    "if pager:\n",
    "    total_pages = int(pager.text.strip().split()[-1])\n",
    "    print(f\"Total pages to scrape: {total_pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books_page(url):\n",
    "    \"\"\"\n",
    "    Scrape all books from a single page.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: URL of the page to scrape\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing book data\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    books_data = []\n",
    "    \n",
    "    # Find all book containers\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        # Extract title\n",
    "        title = book.find('h3').find('a')['title']\n",
    "        \n",
    "        # Extract price\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        \n",
    "        # Extract availability\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "        \n",
    "        # Extract star rating (stored in class name like \"star-rating Three\")\n",
    "        rating_class = book.find('p', class_='star-rating')['class']\n",
    "        rating = rating_class[1]  # Get the second class which is the rating\n",
    "        \n",
    "        books_data.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Availability': availability,\n",
    "            'Rating': rating\n",
    "        })\n",
    "    \n",
    "    return books_data\n",
    "\n",
    "print(\"Scraping function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scrape All Pages with Pagination Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SCRAPING BOOKS FROM ALL PAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_books = []\n",
    "\n",
    "# Scrape all pages\n",
    "for page in range(1, total_pages + 1):\n",
    "    if page == 1:\n",
    "        url = base_url + \"catalogue/page-1.html\"\n",
    "    else:\n",
    "        url = base_url + f\"catalogue/page-{page}.html\"\n",
    "    \n",
    "    print(f\"Scraping page {page}/{total_pages}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        page_books = scrape_books_page(url)\n",
    "        all_books.extend(page_books)\n",
    "        print(f\"Found {len(page_books)} books\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Be respectful - add small delay\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nTotal books scraped: {len(all_books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create DataFrame and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_books = pd.DataFrame(all_books)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BOOKS DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal books: {len(df_books)}\")\n",
    "print(f\"\\nColumns: {df_books.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nRating Distribution:\")\n",
    "print(df_books['Rating'].value_counts())\n",
    "\n",
    "print(\"\\nAvailability Distribution:\")\n",
    "print(df_books['Availability'].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample Data (first 10 books):\")\n",
    "df_books.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df_books.to_csv('books.csv', index=False)\n",
    "print(\"Data exported to 'books.csv' successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: Scrape IMDB Top 250 Movies\n",
    "\n",
    "**Website**: https://www.imdb.com/chart/top/\n",
    "\n",
    "**Task**: Scrape all 250 movies with:\n",
    "1. Rank (1-250)\n",
    "2. Movie Title\n",
    "3. Year of Release\n",
    "4. IMDB Rating\n",
    "\n",
    "**Note**: Use Selenium for dynamic content loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Selenium WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Selenium components\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"Selenium components imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Launch Browser and Navigate to IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SCRAPING IMDB TOP 250 MOVIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment for headless mode\n",
    "\n",
    "# Launch browser\n",
    "print(\"\\nLaunching browser...\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Navigate to IMDB Top 250\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "driver.get(url)\n",
    "print(\"Navigated to IMDB Top 250 page\")\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scroll to Load All Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScrolling to load all movies...\")\n",
    "\n",
    "# Scroll down multiple times to load all content\n",
    "scroll_pause_time = 2\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # Calculate new scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "print(\"Scrolling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse Page Content and Extract Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get page source after all content is loaded\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"Browser closed\")\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Extract movie data\n",
    "movies_data = []\n",
    "\n",
    "movie_list_items = soup.select(\".ipc-metadata-list-summary-item\")\n",
    "print(f\"\\nFound {len(movie_list_items)} movies\")\n",
    "\n",
    "for rank, item in enumerate(movie_list_items, 1):\n",
    "    try:\n",
    "        # Extract title (format: \"1. The Shawshank Redemption\")\n",
    "        title_tag = item.find(\"h3\", class_=\"ipc-title__text\")\n",
    "        full_title = title_tag.text\n",
    "        title = full_title.split(\".\", 1)[1].strip() if \".\" in full_title else full_title\n",
    "        \n",
    "        # Extract year from metadata\n",
    "        metadata_div = item.find(\"div\", class_=\"cli-title-metadata\")\n",
    "        metadata_items = metadata_div.find_all(\"span\", class_=\"cli-title-metadata-item\")\n",
    "        year = metadata_items[0].text.strip() if metadata_items else \"N/A\"\n",
    "        \n",
    "        # Extract rating\n",
    "        rating_span = item.find(\"span\", class_=\"ipc-rating-star\")\n",
    "        rating = rating_span.text.split()[0] if rating_span else \"N/A\"\n",
    "        \n",
    "        movies_data.append({\n",
    "            'Rank': rank,\n",
    "            'Title': title,\n",
    "            'Year': year,\n",
    "            'Rating': rating\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing movie {rank}: {e}\")\n",
    "\n",
    "print(f\"Successfully extracted {len(movies_data)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create DataFrame and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_movies = pd.DataFrame(movies_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMDB TOP 250 MOVIES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal movies: {len(df_movies)}\")\n",
    "print(f\"Columns: {df_movies.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nTop 10 Movies:\")\n",
    "print(df_movies.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 Movies:\")\n",
    "print(df_movies.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df_movies.to_csv('imdb_top250.csv', index=False)\n",
    "print(\"\\nData exported to 'imdb_top250.csv' successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 3: Scrape Weather Information\n",
    "\n",
    "**Website**: https://www.timeanddate.com/weather/\n",
    "\n",
    "**Task**: Scrape weather for top world cities:\n",
    "1. City Name\n",
    "2. Temperature\n",
    "3. Weather Condition (Clear, Cloudy, Rainy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch Weather Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SCRAPING WORLD WEATHER DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# URL for weather page\n",
    "weather_url = \"https://www.timeanddate.com/weather/\"\n",
    "\n",
    "# Set headers to mimic a browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch the page\n",
    "response = requests.get(weather_url, headers=headers)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = []\n",
    "\n",
    "# Find the weather table\n",
    "weather_table = soup.find(\"table\", class_=\"zebra\")\n",
    "\n",
    "if weather_table:\n",
    "    # Find all city rows\n",
    "    city_rows = weather_table.find_all(\"tr\")\n",
    "    \n",
    "    print(f\"\\nFound {len(city_rows)-1} cities in the table\\n\")\n",
    "    \n",
    "    # Skip header row, process data rows\n",
    "    for row in city_rows[1:]:\n",
    "        cells = row.find_all(\"td\")\n",
    "        \n",
    "        if len(cells) >= 3:\n",
    "            try:\n",
    "                # Extract city name\n",
    "                city = cells[0].text.strip()\n",
    "                \n",
    "                # Extract weather condition\n",
    "                weather_cell = cells[1]\n",
    "                img_tag = weather_cell.find('img')\n",
    "                if img_tag and 'title' in img_tag.attrs:\n",
    "                    weather = img_tag['title']\n",
    "                else:\n",
    "                    weather = weather_cell.text.strip()\n",
    "                \n",
    "                # Extract temperature\n",
    "                temperature = cells[2].text.strip()\n",
    "                \n",
    "                weather_data.append({\n",
    "                    'City': city,\n",
    "                    'Weather': weather,\n",
    "                    'Temperature': temperature\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing row: {e}\")\n",
    "else:\n",
    "    print(\"Weather table not found. Trying alternative approach...\")\n",
    "\n",
    "print(f\"Extracted weather data for {len(weather_data)} cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DataFrame and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_weather = pd.DataFrame(weather_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WORLD WEATHER DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal cities: {len(df_weather)}\")\n",
    "print(f\"Columns: {df_weather.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "df_weather.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather condition distribution\n",
    "if len(df_weather) > 0:\n",
    "    print(\"\\nWeather Condition Distribution:\")\n",
    "    print(df_weather['Weather'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df_weather.to_csv('weather.csv', index=False)\n",
    "print(\"\\nData exported to 'weather.csv' successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated three web scraping techniques:\n",
    "\n",
    "**Q1 - Books to Scrape:**\n",
    "- Used `requests` and `BeautifulSoup` for static content\n",
    "- Handled pagination to scrape all 50 pages\n",
    "- Extracted: Title, Price, Availability, Star Rating\n",
    "- Output: `books.csv`\n",
    "\n",
    "**Q2 - IMDB Top 250:**\n",
    "- Used `Selenium` for dynamic JavaScript-rendered content\n",
    "- Implemented scrolling to load all 250 movies\n",
    "- Extracted: Rank, Title, Year, Rating\n",
    "- Output: `imdb_top250.csv`\n",
    "\n",
    "**Q3 - World Weather:**\n",
    "- Used `requests` with custom headers\n",
    "- Parsed HTML table structure\n",
    "- Extracted: City, Weather Condition, Temperature\n",
    "- Output: `weather.csv`\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Use `requests` + `BeautifulSoup` for static HTML content\n",
    "- Use `Selenium` when JavaScript renders content dynamically\n",
    "- Always set appropriate headers and add delays to be respectful\n",
    "- Handle exceptions gracefully for robust scraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
